{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Effnet_TF2_Albumentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "15PInE-CO206cBNi5tYbHiaz_EK_kq6z3",
      "authorship_tag": "ABX9TyMYJl/6UzZDUOWyi4t0QCk1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheikmohdimran/Experiments_2021/blob/main/Vision/Effnet_TF2_Albumentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjjiqRrFh65g"
      },
      "source": [
        "import os\n",
        "import tensorflow\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras import layers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kljE_9EVv7NX"
      },
      "source": [
        "import pandas as pd\n",
        "train_anno=pd.read_csv('/content/drive/MyDrive/Dataset/Annotations/Train Annotations.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzfQaPu_xbCU"
      },
      "source": [
        "files = train_data_dir.rglob('*.jpg')\n",
        "input=pd.DataFrame(files,columns =['a'])\n",
        "input['a']=input['a'].astype(str)\n",
        "df_train=input['a'].str.split('/',expand=True)\n",
        "train=input.join(df_train)[['a',7]]\n",
        "train.columns=('file_path','label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbjHil5y9p4L"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-OmJJM499cS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scr-YkM29p1m"
      },
      "source": [
        "train_df,valid_df= train_test_split(train, test_size=0.20, random_state=42, stratify=train['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VAdD9rDH571"
      },
      "source": [
        "train_data_dir = Path('/content/drive/MyDrive/Dataset/Car Images/Train Images')\n",
        "test_data_dir = Path('/content/drive/MyDrive/Dataset/Car Images/Test Images')\n",
        "\n",
        "BATCH_SIZE = 32 \n",
        "IMG_SIZE = 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je3YBkeW8URb"
      },
      "source": [
        "!pip install -qqq tensorflow_addons\n",
        "!pip install git+https://github.com/mjkvaak/ImageDataAugmentor\n",
        "!pip install -U albumentations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CFuQNM_NqmM",
        "outputId": "bc899f2f-9fbf-4261-ceb3-84d856bbfa99"
      },
      "source": [
        "import tensorflow as tf\n",
        "from ImageDataAugmentor.image_data_augmentor import *\n",
        "import albumentations\n",
        "\n",
        "    \n",
        "AUGMENTATIONS = albumentations.Compose([\n",
        "    albumentations.HorizontalFlip(p=0.5),\n",
        "    albumentations.Rotate(p=0.5,limit=25),\n",
        "    albumentations.OneOf([\n",
        "        albumentations.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.3),\n",
        "        albumentations.RandomBrightnessContrast(brightness_limit=1.5, contrast_limit=0.1)\n",
        "    ],p=1),\n",
        "    #albumentations.HueSaturationValue(p=0.5),\n",
        "    #albumentations.Affine(shear=20),\n",
        "    albumentations.Cutout(num_holes=1, max_h_size=16,max_w_size = 16,p=1),\n",
        "    #albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=15, p=.75),\n",
        "    #albumentations.Blur(blur_limit=3),\n",
        "    #albumentations.OpticalDistortion()\n",
        "    #albumentations.RGBShift(p=0.5),\n",
        "])\n",
        "\n",
        "# dataloaders\n",
        "train_datagen = ImageDataAugmentor(\n",
        "        augment=AUGMENTATIONS,\n",
        "        #validation_split=0.2,\n",
        "        seed=42\n",
        "        )\n",
        "\n",
        "val_datagen = ImageDataAugmentor()#validation_split=0.2)\n",
        "\n",
        "train_ds = train_datagen.flow_from_dataframe(\n",
        "        dataframe=train_df,\n",
        "        directory=train_data_dir,\n",
        "        x_col=\"file_path\",\n",
        "        y_col=\"label\",\n",
        "        target_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical'\n",
        "        )\n",
        "\n",
        "val_ds = val_datagen.flow_from_dataframe(\n",
        "        dataframe=valid_df,\n",
        "        directory=train_data_dir,\n",
        "        x_col=\"file_path\",\n",
        "        y_col=\"label\",\n",
        "        target_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical'\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:690: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 6515 validated image filenames belonging to 196 classes.\n",
            "Found 1629 validated image filenames belonging to 196 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1KbAdSYRXzk"
      },
      "source": [
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOcpNAHnNqj3"
      },
      "source": [
        "def build_model(num_classes=196,IMG_SIZE=224):\n",
        "  inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "  model = tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
        "\n",
        "  # Freeze the pretrained weights\n",
        "  model.trainable = False\n",
        "\n",
        "  # Rebuild top\n",
        "  x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "\n",
        "  top_dropout_rate = 0.5\n",
        "  x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
        "  outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")(x)\n",
        "\n",
        "  # Compile\n",
        "  return tf.keras.Model(inputs, outputs, name=\"EfficientNet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC0yN1LqNqhb",
        "outputId": "1da4c61f-8120-49d5-80e0-70a30efe5343"
      },
      "source": [
        "from functools import partial\n",
        "INIT_LR=0.001\n",
        "MAX_LR=0.1\n",
        "\n",
        "steps_per_epoch = len(train_ds)\n",
        "clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=INIT_LR,\n",
        "                                          maximal_learning_rate=MAX_LR,\n",
        "                                          scale_fn=lambda x: 1/(2.**(x-1)),\n",
        "                                          step_size=2 * steps_per_epoch\n",
        "                                          )\n",
        "\n",
        "\n",
        "optimizer = partial(SGD, momentum=0.9, nesterov=True)(clr)\n",
        "optimizer = tfa.optimizers.SWA(optimizer)\n",
        "\n",
        "model = build_model(num_classes=196,IMG_SIZE=224)\n",
        "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "epochs = 5 \n",
        "\n",
        "\n",
        "hist = model.fit(train_ds, epochs=epochs, validation_data=val_ds, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "204/204 [==============================] - 110s 498ms/step - loss: 7.0500 - accuracy: 0.0416 - val_loss: 4.0545 - val_accuracy: 0.1535\n",
            "Epoch 2/5\n",
            "204/204 [==============================] - 97s 476ms/step - loss: 6.5717 - accuracy: 0.0969 - val_loss: 3.5732 - val_accuracy: 0.1971\n",
            "Epoch 3/5\n",
            "204/204 [==============================] - 97s 476ms/step - loss: 4.4010 - accuracy: 0.1308 - val_loss: 2.9661 - val_accuracy: 0.2971\n",
            "Epoch 4/5\n",
            "204/204 [==============================] - 97s 476ms/step - loss: 3.8863 - accuracy: 0.1959 - val_loss: 2.6550 - val_accuracy: 0.3634\n",
            "Epoch 5/5\n",
            "204/204 [==============================] - 97s 476ms/step - loss: 3.7311 - accuracy: 0.2177 - val_loss: 2.6448 - val_accuracy: 0.3560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj-KnhRSrDGz"
      },
      "source": [
        "checkpoint_path = \"./cp-{iter:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "model.save_weights(checkpoint_path.format(iter=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "933Mp53pqIE6",
        "outputId": "1ce66041-88fd-490f-8787-ac9279dad447"
      },
      "source": [
        "IMG_SIZE = 300\n",
        "\n",
        "train_ds = train_datagen.flow_from_dataframe(\n",
        "        dataframe=train_df,\n",
        "        directory=train_data_dir,\n",
        "        x_col=\"file_path\",\n",
        "        y_col=\"label\",\n",
        "        target_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical',\n",
        "        #subset='training'\n",
        "        )\n",
        "\n",
        "val_ds = val_datagen.flow_from_dataframe(\n",
        "        dataframe=valid_df,\n",
        "        directory=train_data_dir,\n",
        "        x_col=\"file_path\",\n",
        "        y_col=\"label\",\n",
        "        target_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical',\n",
        "        #subset='validation'\n",
        "        )\n",
        "\n",
        "model = build_model(num_classes=196,IMG_SIZE=IMG_SIZE)\n",
        "model.load_weights('./cp-0001.ckpt')\n",
        "\n",
        "optimizer = partial(SGD, momentum=0.9, nesterov=True)(clr)\n",
        "optimizer = tfa.optimizers.SWA(optimizer)\n",
        "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_ds, epochs=epochs, validation_data=val_ds, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6515 validated image filenames belonging to 196 classes.\n",
            "Found 1629 validated image filenames belonging to 196 classes.\n",
            "Epoch 1/5\n",
            "204/204 [==============================] - 120s 549ms/step - loss: 3.6617 - accuracy: 0.2235 - val_loss: 2.5010 - val_accuracy: 0.3622\n",
            "Epoch 2/5\n",
            "204/204 [==============================] - 109s 532ms/step - loss: 3.5904 - accuracy: 0.2347 - val_loss: 2.2937 - val_accuracy: 0.4260\n",
            "Epoch 3/5\n",
            "204/204 [==============================] - 109s 534ms/step - loss: 3.3886 - accuracy: 0.2761 - val_loss: 2.1836 - val_accuracy: 0.4708\n",
            "Epoch 4/5\n",
            "204/204 [==============================] - 111s 543ms/step - loss: 3.2724 - accuracy: 0.2904 - val_loss: 2.1601 - val_accuracy: 0.4843\n",
            "Epoch 5/5\n",
            "204/204 [==============================] - 109s 534ms/step - loss: 3.2679 - accuracy: 0.2958 - val_loss: 2.1739 - val_accuracy: 0.4635\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2e10464b90>"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfczBaEvqIB1"
      },
      "source": [
        "model.save_weights(checkpoint_path.format(iter=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zolw22pH9PuZ",
        "outputId": "728fc9c6-5adf-41e6-ad67-9976a9b30b60"
      },
      "source": [
        "for layer in model.layers[-20:]:\n",
        "  if not isinstance(layer, layers.BatchNormalization):\n",
        "    layer.trainable = True\n",
        "\n",
        "INIT_LR=0.001\n",
        "MAX_LR=0.1\n",
        "\n",
        "steps_per_epoch = len(train_ds)\n",
        "clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=INIT_LR,\n",
        "                                          maximal_learning_rate=MAX_LR,\n",
        "                                          scale_fn=lambda x: 1/(2.**(x-1)),\n",
        "                                          step_size=2 * steps_per_epoch\n",
        "                                          )\n",
        "\n",
        "model.load_weights('./cp-0002.ckpt')\n",
        "\n",
        "model.compile(optimizer=tfa.optimizers.LAMB(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "epochs = 20  \n",
        "model.fit(train_ds, epochs=epochs, validation_data=val_ds, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.awg_optimizer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.awg_optimizer\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.awg_optimizer.decay\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.awg_optimizer.decay\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.awg_optimizer.momentum\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.awg_optimizer.momentum\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.awg_optimizer's state 'momentum' for (root).layer_with_weights-209.gamma\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.awg_optimizer's state 'momentum' for (root).layer_with_weights-209.gamma\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.awg_optimizer's state 'momentum' for (root).layer_with_weights-209.beta\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.awg_optimizer's state 'momentum' for (root).layer_with_weights-209.beta\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.awg_optimizer's state 'momentum' for (root).layer_with_weights-210.kernel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.awg_optimizer's state 'momentum' for (root).layer_with_weights-210.kernel\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.awg_optimizer's state 'momentum' for (root).layer_with_weights-210.bias\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.awg_optimizer's state 'momentum' for (root).layer_with_weights-210.bias\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "204/204 [==============================] - 121s 550ms/step - loss: 3.2230 - accuracy: 0.3028 - val_loss: 1.9066 - val_accuracy: 0.5095\n",
            "Epoch 2/20\n",
            "204/204 [==============================] - 110s 539ms/step - loss: 3.0317 - accuracy: 0.3460 - val_loss: 1.7764 - val_accuracy: 0.5556\n",
            "Epoch 3/20\n",
            "204/204 [==============================] - 110s 540ms/step - loss: 2.8950 - accuracy: 0.3698 - val_loss: 1.6827 - val_accuracy: 0.5617\n",
            "Epoch 4/20\n",
            "204/204 [==============================] - 110s 539ms/step - loss: 2.7600 - accuracy: 0.3997 - val_loss: 1.5979 - val_accuracy: 0.6016\n",
            "Epoch 5/20\n",
            "204/204 [==============================] - 111s 543ms/step - loss: 2.7385 - accuracy: 0.4121 - val_loss: 1.5520 - val_accuracy: 0.5924\n",
            "Epoch 6/20\n",
            "204/204 [==============================] - 110s 539ms/step - loss: 2.6156 - accuracy: 0.4278 - val_loss: 1.4759 - val_accuracy: 0.6188\n",
            "Epoch 7/20\n",
            "204/204 [==============================] - 110s 538ms/step - loss: 2.5199 - accuracy: 0.4553 - val_loss: 1.4400 - val_accuracy: 0.6225\n",
            "Epoch 8/20\n",
            "204/204 [==============================] - 111s 542ms/step - loss: 2.4444 - accuracy: 0.4723 - val_loss: 1.3979 - val_accuracy: 0.6421\n",
            "Epoch 9/20\n",
            "204/204 [==============================] - 110s 539ms/step - loss: 2.4411 - accuracy: 0.4766 - val_loss: 1.3649 - val_accuracy: 0.6470\n",
            "Epoch 10/20\n",
            "204/204 [==============================] - 110s 538ms/step - loss: 2.3523 - accuracy: 0.5031 - val_loss: 1.3275 - val_accuracy: 0.6470\n",
            "Epoch 11/20\n",
            "204/204 [==============================] - 109s 536ms/step - loss: 2.3197 - accuracy: 0.5031 - val_loss: 1.3026 - val_accuracy: 0.6575\n",
            "Epoch 12/20\n",
            "204/204 [==============================] - 109s 535ms/step - loss: 2.2851 - accuracy: 0.5134 - val_loss: 1.2968 - val_accuracy: 0.6501\n",
            "Epoch 13/20\n",
            "204/204 [==============================] - 110s 537ms/step - loss: 2.1917 - accuracy: 0.5299 - val_loss: 1.2805 - val_accuracy: 0.6599\n",
            "Epoch 14/20\n",
            "204/204 [==============================] - 110s 538ms/step - loss: 2.1650 - accuracy: 0.5394 - val_loss: 1.2297 - val_accuracy: 0.6759\n",
            "Epoch 15/20\n",
            "204/204 [==============================] - 110s 536ms/step - loss: 2.1559 - accuracy: 0.5458 - val_loss: 1.2380 - val_accuracy: 0.6685\n",
            "Epoch 16/20\n",
            "204/204 [==============================] - 110s 537ms/step - loss: 2.1281 - accuracy: 0.5497 - val_loss: 1.2107 - val_accuracy: 0.6753\n",
            "Epoch 17/20\n",
            "204/204 [==============================] - 110s 539ms/step - loss: 2.0968 - accuracy: 0.5569 - val_loss: 1.2331 - val_accuracy: 0.6740\n",
            "Epoch 18/20\n",
            "204/204 [==============================] - 109s 535ms/step - loss: 2.0712 - accuracy: 0.5639 - val_loss: 1.2303 - val_accuracy: 0.6673\n",
            "Epoch 19/20\n",
            "204/204 [==============================] - 109s 536ms/step - loss: 2.1168 - accuracy: 0.5630 - val_loss: 1.1859 - val_accuracy: 0.6980\n",
            "Epoch 20/20\n",
            "204/204 [==============================] - 109s 536ms/step - loss: 2.0788 - accuracy: 0.5652 - val_loss: 1.1879 - val_accuracy: 0.6789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiyLaDw79Pr_"
      },
      "source": [
        "model.save_weights(checkpoint_path.format(iter=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZnZHWozRdnl",
        "outputId": "a1412e06-7c10-4dcb-8555-b547a55b12b2"
      },
      "source": [
        "epochs = 10\n",
        "model.fit(train_ds, epochs=epochs, validation_data=val_ds, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "204/204 [==============================] - 110s 537ms/step - loss: 1.9979 - accuracy: 0.5827 - val_loss: 1.1859 - val_accuracy: 0.6826\n",
            "Epoch 2/10\n",
            "204/204 [==============================] - 110s 537ms/step - loss: 1.9366 - accuracy: 0.6011 - val_loss: 1.1717 - val_accuracy: 0.6888\n",
            "Epoch 3/10\n",
            "204/204 [==============================] - 110s 538ms/step - loss: 1.9642 - accuracy: 0.5913 - val_loss: 1.1697 - val_accuracy: 0.6906\n",
            "Epoch 4/10\n",
            "204/204 [==============================] - 110s 538ms/step - loss: 2.0110 - accuracy: 0.5863 - val_loss: 1.1819 - val_accuracy: 0.6863\n",
            "Epoch 5/10\n",
            "204/204 [==============================] - 110s 540ms/step - loss: 1.9438 - accuracy: 0.5997 - val_loss: 1.1565 - val_accuracy: 0.6894\n",
            "Epoch 6/10\n",
            "204/204 [==============================] - 110s 537ms/step - loss: 1.9288 - accuracy: 0.6020 - val_loss: 1.1372 - val_accuracy: 0.6980\n",
            "Epoch 7/10\n",
            "204/204 [==============================] - 110s 539ms/step - loss: 1.9087 - accuracy: 0.6072 - val_loss: 1.1367 - val_accuracy: 0.7004\n",
            "Epoch 8/10\n",
            "204/204 [==============================] - 110s 540ms/step - loss: 1.8927 - accuracy: 0.6100 - val_loss: 1.1008 - val_accuracy: 0.6992\n",
            "Epoch 9/10\n",
            "204/204 [==============================] - 110s 537ms/step - loss: 1.8301 - accuracy: 0.6250 - val_loss: 1.1244 - val_accuracy: 0.7023\n",
            "Epoch 10/10\n",
            "204/204 [==============================] - 110s 537ms/step - loss: 1.8877 - accuracy: 0.6169 - val_loss: 1.1187 - val_accuracy: 0.7004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2e11cd48d0>"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    }
  ]
}